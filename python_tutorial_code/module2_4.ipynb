{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MN is using Kernel Python 3.10.13 for this module\n",
    "\n",
    "# In this weeks Python sessions we’ll look at how we can feed our data into some ML code to get\n",
    "# a decision tree going which can help us with categorisation problems, but in a way that’s maybe,\n",
    "# more visually tractable.\n",
    "\n",
    "# Here’s a link, again, to the google developers course on machine learning, but specifically to\n",
    "# decision trees: https://developers.google.com/machine-learning/decision-forests/decision-trees\n",
    "# It’s not required reading but it’ll help and will clear up things that I’m not always the best\n",
    "# at explaining live, whilst also trying to remember code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['pregancies', 'glu', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/data-to-insight/ERN-sessions/main/data/diabetes_data.csv',\n",
    "                 names=col_names,\n",
    "                 skiprows=1)\n",
    " \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(strategy='mean')\n",
    " \n",
    "error_cols = col_names[1:-1]\n",
    " \n",
    "df[error_cols] =df[error_cols].replace(0, np.nan)\n",
    " \n",
    "for column in error_cols:\n",
    "    df[column] = imp.fit_transform(df[column].values.reshape(-1, 1))\n",
    " \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = col_names[:-1]\n",
    "X = df[features]\n",
    "y = df['label']\n",
    "\n",
    "X.head(10)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/impute.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=1)\n",
    " \n",
    "clf = DecisionTreeClassifier()\n",
    " \n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    " \n",
    "target_values = ['no diabetes', 'diabetes']\n",
    " \n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            xticklabels=target_values,\n",
    "            yticklabels=target_values)\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Real values')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Diabetes precision {51/82*100}')\n",
    "print(f'Diabetes recall {51/85*100}')\n",
    "print()\n",
    "print(f'No Diabetes precision {120/154*100}')\n",
    "print(f'No Diabetes recall {120/151*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "output_report = classification_report(y_test, y_pred, target_names=target_values,\n",
    "                                      output_dict=True)\n",
    " \n",
    "output_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE TERMINAL, run these 3 lines 1-by-1\n",
    "# pip install pydotplus\n",
    "# pip install graphviz\n",
    "# conda install graphviz (not sure about this one)\n",
    "\n",
    "# alternatively, in the codespace write the following:\n",
    "# %pip install graphviz\n",
    "# %pip install pydotplus\n",
    "# %conda install graphviz\n",
    "\n",
    "# \"we have to install graphviz twice, this is how I could get it to work, I don't know why this is the case\" Will\n",
    "# if I do the \"%conda install\" it gives me a new error, so I'm not doing that.\n",
    "\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    " \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf,\n",
    "                out_file=dot_data,\n",
    "                filled=True,\n",
    "                rounded=True,\n",
    "                special_characters=True,\n",
    "                feature_names=features,\n",
    "                class_names=target_values)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "graph.write_png('diabetes.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this last bit of code has a problem, \"GraphViz's executables not found\"\n",
    "\n",
    "# https://github.com/data-to-insight/D2I-Jupyter-Notebook-Tools/blob/main/ml-data%20science%20tutorials/decision%20tree.ipynb\n",
    "\n",
    "# see module2_4b.ipynb where the exercise from Will (at link above) has been copied & tested by MN.\n",
    "# in that test there is still a problem with finding the graphviz executables\n",
    "\n",
    "# Declan is using the same kernel (Python 3.10.13) but he is not having the same Graphviz problems.\n",
    "# The only difference we can see is that in Declan's terminal the prompt does not say:\n",
    "# (.conda) @MarkNunns ➜ /workspaces/Python_ESCC_course (main) $ \n",
    "# Declan's prompt just shows his username, that's it\n",
    "\n",
    "\n",
    "# Verify the environment\n",
    "# https://coderefinery.github.io/installation/conda-environment/\n",
    "\n",
    "# Declan is using the same kernel (Python 3.10.13) in the top right corner.\n",
    "# Also, in the terminal when DF checks the Python version (using: \"python --version\") it says Python 3.10.13.\n",
    "\n",
    "# For MN, it says Python 3.10.13 in the top right corner, but\n",
    "# in the terminal when MN checks the Python version it says 3.12.5\n",
    "# if I can get this changed, the code may start to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy': 0.7186147186147186 ??\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3, criterion='entropy')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "output_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second attribute selection measure we looked at was entropy.\n",
    "# Entropy is a measure of how homogenous the samples in a node are, or how random\n",
    "# the samples in the node are. It can be though of as how messy the data is in a given node,\n",
    "# or how much information our node gives us about the outcomes we want to predict.\n",
    "\n",
    "# High entropy means the system is messy and doesn’t give us a lot of information about a prediction.\n",
    "# When we use entropy as our splitting criteria, which we did for the second plot, this choses\n",
    "# to split nodes will maximise the decrease in average entropy between\n",
    "# nodes (this difference in average entropy is called information gain).\n",
    "# Essentially then, when choosing entropy as our attribute selection method,\n",
    "# we split nodes where, after that split, we have a more homogenous set of samples,\n",
    "# which can be though of as having a better set of samples from which to make a prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
