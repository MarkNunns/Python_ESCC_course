{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes\n",
    "# Dataframes is one of the key ways of working with tabular data in Python, we use it for storing, manipulating, and using data.\n",
    "# To use them, we need to import the package/library that has them to our workspace, this is called pandas.\n",
    "# Every time we use the package, we'd have to tell Python we want to, but calling 'pandas' is a bit long to do over and over,\n",
    "# so the accepted way is to import pandas as pd, and we can use pd as short for pandas.\n",
    "# Pandas is a Python library used for working with data sets.\n",
    "# It has functions for analyzing, cleaning, exploring, and manipulating data.\n",
    "# The name \"Pandas\" has a reference to both \"Panel Data\", and \"Python Data Analysis\"\n",
    "import pandas as pd\n",
    "\n",
    "# Import Numpy as np is a powerful library used to support data analysis and manipulation operations.\n",
    "# Numpy stands for Numerical Python.\n",
    "# It has various uses in terms of structure optimization, data analysis, array mathematics and matrix operations.\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are creating a dataframe by reading a .csv file from a file path     pd.read_csv(<filepath>)\n",
    "filepath = 'https://raw.githubusercontent.com/data-to-insight/csc-validator-be-903/main/tests/fake_data/header.csv'\n",
    "# for a directory filepath (rather than a web link) this CANNOT be read if using the web version of Github.\n",
    "# for reading a directory filepath using GitHub Desktop, put an 'r' in front of the path e.g. filepath = r'https://raw.githubu   '\n",
    "header_fake = pd.read_csv(filepath)\n",
    "# to read an Excel file pd.read_excel(<filepath>)\n",
    "print(header_fake.head(10))\n",
    "print(header_fake.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are creating a fairly basic dataframe. We start by creating a dictionary with 3 values for each key.\n",
    "d = {\"ColumnA\":['ValueX', 2, 'ValueY'],\n",
    "     \"ColumnB\":[4, 5, 6],\n",
    "     \"ColumnC\":['foo', 'bar', 'baz']}\n",
    "\n",
    "df_1  = pd.DataFrame(d)\n",
    "print(df_1)\n",
    "# print(df_1.info())   optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps a more intuitive way of building a Data Frame, one dictionary per row\n",
    "df_2 = pd.DataFrame([\n",
    "    {'letter':'A',\n",
    "     'word':'Foo',\n",
    "     'number':1},\n",
    "     {'letter':'B',\n",
    "     'word':'Bar',\n",
    "     'number':2},\n",
    "     {'letter':'C',\n",
    "     'word':'Baz',\n",
    "     'number':3},\n",
    "])\n",
    "\n",
    "print(df_2)\n",
    "print(df_2.dtypes)\n",
    "# print(df_2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are some operations that can be done on Columns in our dataframe\n",
    "\n",
    "# we can sum strings, not particularly useful here, but there may be circumstances where it is useful...\n",
    "print(df_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously you can't do some numeric functions on strings, so we will look at the 'number' column in the df (using square brackets)\n",
    "# firstly we will find the mean, which is 2.0\n",
    "print(df_2['number'].mean())\n",
    "# cumulative sum is also useful. Notice that the answer it provides is a table, and then sinfo on the Column and Type.\n",
    "print(df_2['number'].cumsum())\n",
    "# Min and max. Notice we can access a column using . notation rather than square braces if we want, though square brackets is more common\n",
    "print([df_2['number'].min(), df_2.number.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# briefly introducing some functions and conditionals e.g. mapping\n",
    "\n",
    "# for example, here we create an additional calculated field called 'product 3' in our df\n",
    "df_2['product 3'] = df_2['number'] * 3\n",
    "# we can also perform actions on columns easily, for example, multiplying each row in the number column by 3 to make a new column\n",
    "print(df_2)\n",
    "\n",
    "# here we create an another calculated field called 'multiple_of_3' in our df, this is done in 2 stages:\n",
    "# Stage 1: we define the calculation\n",
    "def multiple_of_3(x):\n",
    "    if x % 3:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "# Stage 2: we create the new field. We use mapping .map() if we want to perform a defined action on every row.\n",
    "df_2['multiple of 3'] = df_2['number'].map(multiple_of_3)\n",
    "print(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a new dataframe to work with...\n",
    "staff = pd.DataFrame([\n",
    "   {'Name':'Will Levack-Payne',\n",
    "    'Salary':'35000',\n",
    "    'Job Title':'Teacher'},\n",
    "    {'Name':'Annie Levack-Payne',\n",
    "    'Salary':'52000',\n",
    "    'Job Title':'Head Teacher'},\n",
    "   {'Name':'Andy Levack-Payne',\n",
    "    'Salary':'12000',\n",
    "    'Job Title':'Trainee Teacher'},\n",
    "    {'Name':'Naiomi Levack-Payne',\n",
    "    'Salary':'24000',\n",
    "    'Job Title':'Sports coach'},\n",
    "    {'Name':'Scout Dog',\n",
    "    'Salary':'0',\n",
    "    'Job Title':'Dog'}\n",
    "])\n",
    "print(staff['Salary'])\n",
    "print(staff[['Name','Job Title']])\n",
    "# print(staff.info())    optional\n",
    "staff['Salary'] = staff['Salary'].astype('int')\n",
    "print(staff['Salary'].dtypes)\n",
    "mean_salary = staff['Salary'].mean()\n",
    "sum_salary = staff['Salary'].sum()\n",
    "cumsum_salary = staff['Salary'].cumsum()\n",
    "max_salary = staff['Salary'].max()\n",
    "min_salary = staff['Salary'].min()\n",
    "print(f'Mean salary is: {mean_salary}')\n",
    "print(f'Total of salaries is: {sum_salary}')\n",
    "print(f'Cumulative Sum of salaries is: {cumsum_salary}')\n",
    "print(f'Max and Min salaries respectively are {max_salary}, {min_salary}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone at the company has had a pay rise. Update the salary column to reflect this by using the * operator by 1.07,\n",
    "# and setting it equal to the same column. Print the new column using the square bracket method.\n",
    "# Use the .map method and the pre-written function (below) on your dataframe to make a new column indicating whose salary\n",
    "# is above average (The average uk salary is Â£33,402). \n",
    "\n",
    "def above_average(x):\n",
    "    if x > 33402:\n",
    "        return 'Above average'\n",
    "    if x == 33402:\n",
    "        return 'Average'\n",
    "    else:\n",
    "        return 'Below average'\n",
    "staff['comparison'] = staff['Salary'].map(above_average)\n",
    "print(staff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Add a start date column to the dataframe of employees (staff). Print the updated df.\n",
    "# Using DataFrame.insert() to add a column\n",
    "staff = staff.assign(start_date=['19/02/2012','04/01/2005', '15/11/2018', '27/08/2021', '23/10/2017'])\n",
    "print(staff)\n",
    "# Convert it to a datetime data type.\n",
    "# if '19/02/2012' is the 1st date entered then it all works OK, but not if '04/01/2005' is the 1st date entered.\n",
    "# we use the 'format=' to deal with this\n",
    "staff['start_date']= pd.to_datetime(staff['start_date'], format=\"%d/%m/%Y\")\n",
    "# source: https://sparkbyexamples.com/pandas/pandas-convert-string-column-to-datetime/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for years worked.\n",
    "staff['days_worked'] = pd.to_datetime('18/01/2024', dayfirst=True) - staff['start_date']\n",
    "# It's now a time delta object (in days), we want this in years.\n",
    "\n",
    "# again, if we use 08/01/2024 instead of 18/01/2024 we get problems with the date format\n",
    "import numpy as np\n",
    "staff['years_worked'] = staff['days_worked'] / pd.Timedelta('365 days')\n",
    "staff['years_worked'] = staff['years_worked'].round().astype('int', errors='ignore')\n",
    "# Round, convert to integers, ignoring the error produced by the empty value for age\n",
    "print(staff)\n",
    "\n",
    "# Use the .mean() method on the years worked column to calculate the average time people have worked for and assign this for a variable.\n",
    "mean_years_worked = staff['years_worked'].mean()\n",
    "print(f'Average years worked is {mean_years_worked}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe by copying the previous Dataframe, give it an appropriate name\n",
    "staff2 = staff[['Name','Salary','Job Title','years_worked']].copy()\n",
    "# Choose only rows where the years worked value is greater than the average.\n",
    "staff2 = staff2[staff2['years_worked'] > mean_years_worked]\n",
    "# An easy way to make a new column and add values to it, is to have the dataframe column and set it equal\n",
    "# to a list of values you want, in order.\n",
    "# For example 'df['numbers'] = [1, 2, 3, 4, 5]'. Make sure you have as many items in your list as you have rows in the dataframe.\n",
    "# If the new column is dates, add the dates as strings, and all in the same format (eg dd/mm/yyyy).\n",
    "staff2['extra_date'] = ['28/05/2023', '30/09/2023']\n",
    "# then convrt the new column containing dates to date format. \n",
    "staff2['extra_date'] = pd.to_datetime(staff2['extra_date'], format=\"%d/%m/%Y\")\n",
    "print(staff2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell use the .str.lower() method to make all the names in the employee names column lower case\n",
    "staff2['Name'] = staff2['Name'].str.lower()\n",
    "# You now need to make two new columns, one for first and one for last name.\n",
    "# Use this stack overflow link to learn how to split columns in pandas: \n",
    "# https://stackoverflow.com/questions/38437847/pandas-split-name-column-into-first-and-last-name-if-contains-one-space\n",
    "# You can use str.split to split the strings, then test the number of splits using str.len and use this as a boolean mask\n",
    "# to assign just those rows with the last component of the split:\n",
    "staff2.loc[staff2['Name'].str.split().str.len() == 2, 'first name'] = staff2['Name'].str.split().str[0]\n",
    "staff2.loc[staff2['Name'].str.split().str.len() == 2, 'last name'] = staff2['Name'].str.split().str[-1]\n",
    "staff2['ID'] = ['A803', 'A807']\n",
    "print(staff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, make a dataframe for a new employee, with all the same columns as before, and an extra 'employee ID' column.\n",
    "extra_list = {'Name':['Bob Peck'],'Salary':[40000],'Job Title':['cowboy'],'years_worked':['3'],'extra_date':[''],\n",
    "              'first name':['Bob'],'last name':['Peck'],'ID':['A704']}\n",
    "extra_staff  = pd.DataFrame(extra_list)\n",
    "print(extra_staff)\n",
    "# Use the pd.concat() method to join it to your employees dataframe.\n",
    "staff3 = pd.concat([staff2, pd.DataFrame(extra_staff)], ignore_index=True)\n",
    "print(staff3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe of your employees names and some employee IDs, then merge it with your big dataframe,\n",
    "# and use the drop, fillna, and rename methods to clean it up.\n",
    "# To finish off, make a new dataframe with every employee from your large employees dataframe with a column for\n",
    "# their employee IDs, and a new column for their managers. \n",
    "# Merge this to the bigger dataframe and print it to see that merging doesn't always need any clean up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session 4 start for real\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'ChildId':['id1', 'id2', 'id3', 'id4', 'id5'],\n",
    "                  'Age first contact':[6,12,11,1,19],\n",
    "                  'Gender':['M','m', 'F', '', 'F' ],\n",
    "                  'Birthday':['01/01/2002', '02/02/2003', pd.NA, '03/03/2023', '06/01/2012'],\n",
    "                  'CP Plan?':['Y', 'n', 'N', 'No', 'yES'],})\n",
    "print(df.info())\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this new df to go though some key DF processes:\n",
    "# a) making selections based on criteria\n",
    "# b) some basic data cleaning\n",
    "# c) making sure data is of the right type\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_genders = df['Gender'].unique - what was this?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map function is a bit like an IF function in Excel.\n",
    "# The Python 'map' method is worth knowing about, but there are more efficient methods.\n",
    "gender_dict = {'M':'m',\n",
    "               'm':'m',\n",
    "               'F':'f',\n",
    "               'f':'f'}\n",
    "df['Gender'] = df['Gender'].map(gender_dict)\n",
    "# this next line does the same thing but better\n",
    "df['Gender'] = df['Gender'].str.lower()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we deal with NA values in the Gender column\n",
    "df['Gender'] = df['Gender'].fillna(pd.NA)\n",
    "df['Gender'] = df['Gender'].replace('', pd.NA)\n",
    "# the following line uses a 'regular expression' instead of a blank space.\n",
    "# the regular expression for a blank space is:  r'^\\s*4'\n",
    "df['Gender'] = df['Gender'].replace(r'^\\s*4', pd.NA, regex=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate age at 31/03/2024\n",
    "# making sure stuff is the right type: our dates are strings, we need them to be dates\n",
    "df['Birthday'] = pd.to_datetime(df['Birthday'], format='%d/%m/%Y', errors = 'coerce')\n",
    "# Now it's a datetime object, we can do a calculation to work out current ages!\n",
    "df['Age March 31'] = pd.to_datetime('31/03/2024',dayfirst=True) - df['Birthday']\n",
    "# It's now a time delta object, we want age in years!\n",
    "df['Age March 31'] = df['Age March 31'] / pd.Timedelta('365 days')\n",
    "# next line rounds, converts to integers, ignores the error produced by the empty value for age.\n",
    "df['Age March 31'] = df['Age March 31'].round(1).astype('int', errors = 'ignore')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate age at today\n",
    "df['Age'] = pd.to_datetime('today').normalize() - df['Birthday']\n",
    "df['Age'] = df['Age'] / pd.Timedelta('365 days')\n",
    "# next line rounds, converts to integers, ignores the error produced by the empty value for age.\n",
    "df['Age'] = df['Age'].round().astype('int', errors = 'ignore')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do an error check and take rows where first contact is an age older then their current age\n",
    "error_df = df[df['Age first contact'] > df['Age']]\n",
    "print(error_df) # We can see, based on our selection, child id5 has an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've converted to dates, calculated ages, and found a row with errors, lets do a little cleaning now.\n",
    "\n",
    "# Convert all gender values to lower case\n",
    "df['Gender'] = df['Gender'].str.lower()\n",
    "\n",
    "# convert all cp plan? to lower... but we still need the right things in each column\n",
    "df['CP Plan?'] = df['CP Plan?'].str.lower()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the values in the column 'CP Plan?'. We are going to tidy up this mess here.\n",
    "# We will use a lambda function to change values based on other values.\n",
    "# this next example of lambda usage is a bit like the map function we used earlier.\n",
    "cp_cleaner = lambda row : 'y' if 'y' in row else 'n' if 'n' in row else pd.NA\n",
    "df['CP Plan?'] = df['CP Plan?'].str.lower().apply(cp_cleaner)\n",
    "\n",
    "# .fillna() can be used to fill empty rows with whatever we want.\n",
    "df = df.fillna(pd.NA)\n",
    "# The standard would be pd.NA which pandas understands as NAs\n",
    "# Note, this hasn't worked for our empty value in gender as an empty string is still a string!\n",
    "\n",
    "# replace field that's entirely space (or empty) with NaN\n",
    "df = df.replace(r'^\\s*$', pd.NA, regex=True)\n",
    "# The easiest way to fill it is replacing the regex expression for an empty string with an na.\n",
    "# We can see in the table below we have a NaN, an <NA>, and an NaT.\n",
    "# These are the Not a Number, Empty, and Not a Time NAs for each data type.\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question:  what is the difference between a cp_cleaner lambda function and a for loop?\n",
    "# This 'for loop' method is messier than using a panda method to loop\n",
    "# for i in range len(df):\n",
    "#     i = i-1\n",
    "#     value = df['CP Plan?'].iloc [i]\n",
    "#    if value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say we have another child\n",
    "new_child = {\n",
    "    'ChildId':['id6'],\n",
    "    'Current Age': [10],\n",
    "    'Age first contact':[9],\n",
    "    'Gender':['m'],\n",
    "    'Birthday':[pd.to_datetime('28/02/2014', dayfirst=True)],\n",
    "    'Age at End of Reporting Period':[10],\n",
    "    'NHS Number':['78132']\n",
    "}\n",
    "print(new_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we join df and new_child dataframes using concat function\n",
    "import pandas as pd\n",
    "new_child = pd.DataFrame(new_child)\n",
    "df = pd.concat([df, new_child], ignore_index=True)\n",
    "# then to remove duplicate rows we do...\n",
    "df.drop_duplicates(subset='ChildId', keep='last', inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are making a new dataframe here, then we will try some new functions are being tried\n",
    "nhs_numbers = pd.DataFrame([\n",
    "                            {'ChildId':'id1',\n",
    "                            'NHS Number': '303',},\n",
    "                            {'ChildId':'id2',\n",
    "                            'NHS Number': '3u5029',},\n",
    "                            {'ChildId':'id3',\n",
    "                            'NHS Number': 'gqw3',},\n",
    "                            {'ChildId':'id4',\n",
    "                            'NHS Number': 'avsgvb',},\n",
    "                            {'ChildId':'id5',\n",
    "                            'NHS Number': 'varwvw',},\n",
    "                            ])\n",
    "print(nhs_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are going to join the df dataframe with the nhs_numbers dataframe using the merge function.\n",
    "# Very like SQL joins, there are inner merge, outer merge, full merge.\n",
    "df = pd.merge(df, nhs_numbers, how='left', on='ChildId')\n",
    "# The field NHS Number appeared in both the tables being merged.\n",
    "# Notice how, after the merge, we now have 2 NHS Number fields.\n",
    "# Python has called these fields NHS Number_x (field from df) and NHS Number_y (field from nhs_numbers)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next line looks for any NA values in column NHS Number_x, and replaces them with the value from NHS Number_y.\n",
    "df['NHS Number_x'] = df['NHS Number_x'].fillna(df['NHS Number_y'])\n",
    "# next line loses the NHS Number_y column from the dataframe.\n",
    "df.drop('NHS Number_y', axis=1, inplace=True)\n",
    "# next line renames the NHS Number_x column.\n",
    "df.rename({'NHS Number_x': 'NHS Number'}, axis=1, inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session 2 Group Work\n",
    "# There is a csv of house price data in the UK from 1980-2023 here:\n",
    "# https://raw.githubusercontent.com/data-to-insight/D2I-Jupyter-Notebook-Tools/main/ERN-worksheets/data/1980%202023%20average%20house%20prices.csv\n",
    "\n",
    "# This is real data taken from the ONS website, but stored on GitHub for ease of accessibility.\n",
    "\n",
    "# read data from a url into pandas, and do so, naming the dataframe variable appropriately.\n",
    "# Convert date columns to datetime type.\n",
    "# Find the year with the highest and lowest house prices.\n",
    "# Find the year with the lhighest and lowest percentage increase in house prices.\n",
    "# Slice the dataframe to return only rows where the average house price is over 200,000.\n",
    "# Slice the dataframe to return only rows where the average house price in that year is over the median house price for all the data.\n",
    "\n",
    "# Now for the hard bit once again:\n",
    "# work out how to plot the average house price against the time period as a line plot, using a library called seaborn.\n",
    "# You will need to import seaborn as sns (just as we import pandas as pd).\n",
    "# A good place to start is google, and the seaborn documentation: https://seaborn.pydata.org/generated/seaborn.lineplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "housefile = 'https://raw.githubusercontent.com/data-to-insight/D2I-Jupyter-Notebook-Tools/main/ERN-worksheets/data/1980%202023%20average%20house%20prices.csv'\n",
    "dfh = pd.read_csv(housefile)\n",
    "# Convert date columns to datetime type\n",
    "dfh['Period'] = pd.to_datetime(dfh['Period'], format='%Y-%m', errors='coerce')\n",
    "print(dfh.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the year with the highest and lowest house prices.\n",
    "# Find the year with the lhighest and lowest percentage increase in house prices.\n",
    "MNimax = dfh['Average price All property types'].idxmax()\n",
    "print(dfh.loc[MNimax, 'Period'], dfh.loc[MNimax, 'Average price All property types'], f'(index position {MNimax})')\n",
    "MNimin = dfh['Average price All property types'].idxmin()\n",
    "print(dfh.loc[MNimin, 'Period'], dfh.loc[MNimin, 'Average price All property types'], f'(index position {MNimin})')\n",
    "MNimax2 = dfh['Percentage change (yearly) All property types'].idxmax()\n",
    "print(dfh.loc[MNimax2, 'Period'], dfh.loc[MNimax2, 'Percentage change (yearly) All property types'], f'(index position {MNimax2})')\n",
    "MNimin2 = dfh['Percentage change (yearly) All property types'].idxmin()\n",
    "print(dfh.loc[MNimin2, 'Period'], dfh.loc[MNimin2, 'Percentage change (yearly) All property types'], f'(index position {MNimin2})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the dataframe to return only rows where the average house price is over 200,000.\n",
    "over_200k = dfh['Average price All property types'] > 200000\n",
    "sliced_dfh1 = dfh[over_200k]\n",
    "print(sliced_dfh1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the dataframe to return only rows where the average house price in that year is over the median house price for all the data.\n",
    "med = dfh['Average price All property types'].median()\n",
    "over_med = dfh['Average price All property types'] > med\n",
    "sliced_dfh2 = dfh[over_med]\n",
    "print(f'The median average price is {med}')\n",
    "print(sliced_dfh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average house price against the time period as a line plot\n",
    "# for graphing work we will use a Python library called seaborn.\n",
    "import seaborn as sns\n",
    "sns.lineplot(data=(dfh), x=\"Period\", y=\"Average price All property types\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
