{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These session notes come from the code that Will pasted into the Teams chat\n",
    "\n",
    "# The data used in this session comes from here:\n",
    "# https://explore-education-statistics.service.gov.uk/find-statistics/children-in-need\n",
    "\n",
    "# This is the code for the course2-session8 that Will published on Github, as opposed to the Teams chat code.\n",
    "# https://github.com/data-to-insight/ERN-sessions/blob/main/No%20Local%20Python/session_1_data_wrangling.py\n",
    "\n",
    "# Will has put the datasets into csv files in his Github location here:\n",
    "# https://github.com/data-to-insight/ERN-sessions/tree/main/No%20Local%20Python/data\n",
    "# these files need to be copied to MN's Github codespace (if using Github),   or\n",
    "# these files need to be copied to MN's D:\\ drive, if using MN's PC and the VS Code software.\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# We can use glob to identify all the files at a given filepath.\n",
    "# Below is a nice way to do this, we also use *.csv to say we want ONLY files matching this criteria\n",
    "# path = r'/workspaces/ERN-sessions/No Local Python/data'\n",
    "path = r'/workspaces/Python_ESCC_course/python_tutorial_code/course_2/data_mod2_8-9'\n",
    "files = glob.glob(path + \"/*.csv\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an empty dictionary to store our dataframes in\n",
    "dfs = {}\n",
    "\n",
    "# Glob returns a list of file paths so we can iterate over \n",
    "# the list to read one file at a time\n",
    "for f in files:\n",
    "    # For each file in the list, read them in using the  \n",
    "    # standard/appropriate pandas method\n",
    "    df = pd.read_csv(f)\n",
    "    # Glob returns strings of file paths, we can use the\n",
    "    # split method on these, passing what character we want to split at\n",
    "    # then selecting which element of that split we want.\n",
    "    # Here I take the last element after the \"/\" split (the file name)\n",
    "    # using [-1] and the first element after the \"_\" split using [0]\n",
    "    # to get the list short name (eg: a1) \n",
    "    list_name = f.split(\"/\")[-1][:-17]#.split(\"_\")[0]\n",
    " \n",
    "    # I can then use the variables list_name and df as they key/values\n",
    "    # in my dictionary to store all the dfs in one easy to access place\n",
    "    # using the syntax below where list_name is a string, and df is the df,\n",
    "    # using the square brackets to say:\n",
    "    # 'in the dfs dictionary, I want the value associated with the string in the\n",
    "    # square brackets (the key) to be what's after the equals'.\n",
    "    dfs[list_name] = df\n",
    "    # print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's sort the dictionary to make things look like we expect later by making a \n",
    "# dictionary using a list comprehension\n",
    "dfs = {key:dfs[key] for key in sorted(dfs.keys())}\n",
    "\n",
    "# We need a df to start merging on to, and to get the columns to merge on from\n",
    "left_df = dfs['b1_children_in_need']\n",
    "# print(left_df)\n",
    "\n",
    "# Get a list of the columns that are the same in every table.\n",
    "# This saves writing them out by hand and if they change with publication years\n",
    "# you can extract them regardless of spelling.\n",
    "permenant_columns = list(left_df.columns[:10])\n",
    "print(permenant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also add the table name as a prefix to the columns to match the original\n",
    "# data and help merging later\n",
    "left_df = left_df.set_axis([f'b1_children_in_need_{column}' if (not column in permenant_columns) else column for column in left_df.columns], axis=1)\n",
    "# print(left_df.columns)\n",
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets go through all the dfs we want and merge them into the mega table we want\n",
    "left_df = left_df.merge(dfs['b2_children_in_need_recorded_disability'], how='outer', on=permenant_columns)\n",
    "\n",
    "for key, df in dfs.items():\n",
    "    if ('headline_figures' not in key) & (key[:1] != 'b1') & ('mid-year' not in key) & (key[0] != 'a'):\n",
    "        # Add the table name prefix to each colum again (important to understand table\n",
    "        # data and to help the merge)\n",
    "        df = df.set_axis([f'{key}_{column}' if (not column in permenant_columns) else column for column in df.columns], axis=1)\n",
    "        print(df.columns)\n",
    "        # Each loop we'll merge onto the previous left_table, gradually building\n",
    "        # it up. We'll use a left merge to add the table onto the right of the previous table\n",
    "        # We'll also merge on permenant_columns to a) get the right rows merged and b) so we\n",
    "        # don't end up with multiple versions of the same columns from different tables.\n",
    "        left_df = left_df.merge(df, how='left', on=permenant_columns)\n",
    "\n",
    "        # We'll see that this doesn't work yet because column names are repeated \n",
    "        # across tables, let's maybe get table names in each column\n",
    "\n",
    "print(left_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
