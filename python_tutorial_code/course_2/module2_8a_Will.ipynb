{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'b1_children_in_need'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m dfs \u001b[38;5;241m=\u001b[39m {key:dfs[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(dfs\u001b[38;5;241m.\u001b[39mkeys())}\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# We need a df to start merging on to, and to get the columns to merge on from\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m left_df \u001b[38;5;241m=\u001b[39m \u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb1_children_in_need\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Get a list of the columns that are the same in every table.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# This saves writing them out by hand and if they change with publication years\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# you can extract them regardless of spelling.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m permenant_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(left_df\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b1_children_in_need'"
     ]
    }
   ],
   "source": [
    "# For session 8, Will's published code on Github came in 2 files, I am calling this first file '8a'\n",
    "# Will had called his 1st file:    \"session_1_data_wrangling.py\"\n",
    "# Keep this code as published by Will, do not change it. There is a course2_session8_MN file where we make changes & notes.\n",
    "\n",
    "# data here: https://explore-education-statistics.service.gov.uk/find-statistics/children-in-need\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# We can use glob to identify all the files at a given filepath\n",
    "# Below is a nice way to do this, we also use *.csv to say we want\n",
    "# ONLY files mathcing this criteria\n",
    "path = r'/workspaces/ERN-sessions/No Local Python/data'\n",
    "files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Set up an empty dictionary to store our dataframes in\n",
    "dfs = {}\n",
    "\n",
    "# Glob returns a list of file paths so we can iterate over \n",
    "# the list to read one file at a time\n",
    "for f in files:\n",
    "    # For each file in the list, read them in using the  \n",
    "    # standard/appropriate pandas method\n",
    "    df = pd.read_csv(f)\n",
    "    # Glob returns strings of file paths, we can use the\n",
    "    # split method on these, passing what character we want to split at\n",
    "    # then selecting which element of that split we want.\n",
    "    # Here I take the last element after the \"/\" split (the file name)\n",
    "    # using [-1] and the first element after the \"_\" split using [0]\n",
    "    # to get the list short name (eg: a1) \n",
    "    list_name = f.split(\"/\")[-1][:-17]#.split(\"_\")[0]\n",
    " \n",
    "    # I can then use the variables list_name and df as they key/values\n",
    "    # in my dictionary to store all the dfs in one easy to access palce\n",
    "    # using the syntax below where list_name is a string, and df is the df\n",
    "    # using the square brackets to say 'in the dfs dict, I want the value\n",
    "    # associated with the string in the square brackets (the key) to be  \n",
    "    # what's after the equals'\n",
    "    dfs[list_name] = df\n",
    "\n",
    "# Let's sort the dictionary to make things look like we expect later by making a \n",
    "# dictionary using a list comprehension\n",
    "dfs = {key:dfs[key] for key in sorted(dfs.keys())}\n",
    "\n",
    "# We need a df to start merging on to, and to get the columns to merge on from\n",
    "left_df = dfs['b1_children_in_need']\n",
    "\n",
    "# Get a list of the columns that are the same in every table.\n",
    "# This saves writing them out by hand and if they change with publication years\n",
    "# you can extract them regardless of spelling.\n",
    "permenant_columns = list(left_df.columns[:10])\n",
    "\n",
    "# Let's also add the table name as a prefix to the columns to match the original\n",
    "# data and help merging later\n",
    "left_df = left_df.set_axis([f'b1_children_in_need_{column}' if (not column in permenant_columns) else column for column in left_df.columns], axis=1)\n",
    "\n",
    "# print(left_df.columns)\n",
    "# sys.exit()\n",
    "# lets go through all the dfs we want and merge them into the mega table we want\n",
    "\n",
    "left_df = left_df.merge(dfs['b2_children_in_need_recorded_disability'], how='outer', on=permenant_columns)\n",
    "\n",
    "for key, df in dfs.items():\n",
    "    if ('headline_figures' not in key) & (key[:1] != 'b1') & ('mid-year' not in key) & (key[0] != 'a'):\n",
    "        # Add the table name prefix to each colum again (important to understand table\n",
    "        # data and to help the merge)\n",
    "        df = df.set_axis([f'{key}_{column}' if (not column in permenant_columns) else column for column in df.columns], axis=1)\n",
    "        print(df.columns)\n",
    "        # Each loop we'll merge onto the previous left_table, gradually building\n",
    "        # it up. We'll use a left merge to add the table onto the right of the previous table\n",
    "        # We'll also merge on permenant_columns to a) get the right rows merged and b) so we\n",
    "        # don't end up with multiple versions of the same columns from different tables.\n",
    "        left_df = left_df.merge(df, how='left', on=permenant_columns)\n",
    "\n",
    "        # We'll see that this doesn't work yet because column names are repeated \n",
    "        # across tables, let's maybe get table names in each column\n",
    "\n",
    "print(left_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
